# List of Bloomberg tickers and their corresponding readable names
# tickers = ['LBUSTRUU Index', 'H0A0 Index', 'SPX Index', 'SPBDAL Index', 'SX5E Index', 'NKY Index', 'CL1 Comdty', 'XAUUSD Curncy', 'EURUSD Curncy', 'VNQ US Equity']
# readable_names = ['US Aggregate Bonds', 'US High Yield Bonds', 'US Equities', 'US Bank Loans', 'European Equities', 'Asian Equities', 'Oil', 'Gold', 'EUR/USD', 'US Real Estate']






==================================

events_df = pd.DataFrame({
    'Event': ['Lehman Collapse', 'Brexit Vote', 'COVID-19 Pandemic', 'ECB QE Announcement', 'Russia-Crimea Annexation'],
    'Date': ['2008-09-15', '2016-06-23', '2020-03-11', '2015-01-22', '2014-03-18']
})

events_df['Start_Date'] = pd.to_datetime(events_df['Date']) - pd.Timedelta(days=5)
events_df['End_Date'] = pd.to_datetime(events_df['Date']) + pd.Timedelta(days=5)

print(events_df)


==========================================================================





# Define events
events = [
    {"name": "Lehman Collapse", "start": "2008-09-10", "end": "2008-09-20"},
    {"name": "Brexit Vote", "start": "2016-06-18", "end": "2016-06-28"},
    {"name": "COVID-19 Pandemic", "start": "2020-03-06", "end": "2020-03-16"},
    {"name": "ECB QE Announcement", "start": "2015-01-17", "end": "2015-01-27"},
    {"name": "Russia-Crimea Annexation", "start": "2014-03-13", "end": "2014-03-23"}
]

# Function to highlight strong correlations in heatmap
def highlight(val):
    if val > 0.75:
        return 'background-color: yellow'
    elif val < -0.75:
        return 'background-color: red'
    else:
        return ''

for event in events:
    subset = merged_df.loc[event["start"]:event["end"]]
    corr = subset.corr()
    matrix = np.triu(corr)

    
    # 1. Highlight strong correlations in heatmap
    plt.figure(figsize=(12, 8))
    styled_cm = correlation_matrix.style.applymap(highlight)
    sns.heatmap(correlation_matrix, annot=True, mask=matrix, cmap='coolwarm', vmin=-1, vmax=1)
    plt.title(f"Correlation during {event['name']}")
    plt.show()
    # print(correlation_matrix)

    # 2. Compute and plot rolling correlations
    rolling_corr = merged_df['Sentiment Score'].rolling(window=180).corr(merged_df['US Large Cap Equities'])
    plt.figure(figsize=(12, 6))
    rolling_corr.plot()
    plt.title(f"Rolling Correlation between Sentiment Score and US Large Cap Equities during {event['name']}")
    plt.xlabel("Date")
    plt.ylabel("Rolling Correlation")
    plt.tight_layout()
    plt.show()

    # 3. Overlay sentiment scores on market return charts
    fig, ax1 = plt.subplots(figsize=(12, 6))
    color = 'tab:red'
    ax1.set_xlabel('Date')
    ax1.set_ylabel('Sentiment Score', color=color)
    ax1.plot(merged_df.index, merged_df['Sentiment Score'], color=color)
    ax1.tick_params(axis='y', labelcolor=color)

    ax2 = ax1.twinx()
    color = 'tab:blue'
    ax2.set_ylabel('US Large Cap Equities', color=color)
    ax2.plot(merged_df.index, merged_df['US Large Cap Equities'], color=color)
    ax2.tick_params(axis='y', labelcolor=color)

    fig.tight_layout()
    plt.title(f"Sentiment Scores & US Large Cap Equities during {event['name']}")
    plt.show()
    
    
========================================================================

df = index_data

event_returns = {}

for _, event in events_df.iterrows():
    mask = (df.index >= event['Start_Date']) & (df.index <= event['End_Date'])
    window_df = df[mask]
    
    average_returns = window_df.pct_change().mean()
    event_returns[event['Event']] = average_returns

event_returns_df = pd.DataFrame(event_returns)


non_event_mask = np.logical_not(
    np.any(
        [((df.index >= start_date) & (df.index <= end_date)) for start_date, end_date in zip(events_df['Start_Date'], events_df['End_Date'])],
        axis=0
    )
)

non_event_df = df[non_event_mask]
normal_returns = non_event_df.pct_change().mean()

event_returns_df.T.plot(kind='bar', figsize=(15,7))
plt.title('Average Returns During Event Windows')
plt.ylabel('Returns')
plt.xlabel('Events')
plt.legend(title='Indexes')
plt.tight_layout()
plt.show()

# Combining the event returns with normal returns
all_returns_df = event_returns_df.copy()
all_returns_df['Non-Event'] = normal_returns


def color_gradient(val):
    color = ''
    if val > 0:
        color = f'rgb(0, {int(255*(1-val))}, 0)' # Gradient of green for positive values
    else:
        color = f'rgb({int(255*(1+val))}, 0, 0)' # Gradient of red for negative values        
    return f'background-color: {color};'

styled_table = all_returns_df.style.applymap(color_gradient)

# Custom styles
styles = {
    'Non-Event': 'border-right: 3px solid black'
}

def format_percentage(val):
    return f'{val * 100:.2f}%'

styled_table = all_returns_df.style.format(format_percentage) \
                                 .background_gradient(cmap=sns.diverging_palette(250, 15, s=75, l=40, as_cmap=True), axis=0) 

styled_table



================================

# Import necessary libraries
from textblob import TextBlob
import nltk

from nltk.corpus import stopwords
from gensim.models import LdaModel
from gensim.corpora import Dictionary
import os

def load_articles_from_directory(directory_path):
    """
    Load all .txt files from the given directory.
    
    Args:
    - directory_path (str): Path to the directory containing .txt files.
    
    Returns:
    - dict: A dictionary with filenames as keys and their content as values.
    """
    articles = {}

    # Loop through each file in the directory
    for filename in os.listdir(directory_path):
        if filename.endswith(".txt"):
            file_path = os.path.join(directory_path, filename)
            
            try:
                # Try reading with utf-8 encoding first
                with open(file_path, 'r', encoding='utf-8') as file:
                    articles[filename] = file.read()
            except UnicodeDecodeError:
                # If utf-8 fails, try with latin1 encoding
                with open(file_path, 'r', encoding='latin1') as file:
                    articles[filename] = file.read()

    return articles

# Example usage
articles = load_articles_from_directory("C:\\Users\\Scott Morgan\\OneDrive\\1_Professional_Info\\Projects\\nlp-quantitative-finance\\articles")
# print(articles.keys())  # This will print the names of the loaded files

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Preprocess text
def preprocess(text):
    tokens = nltk.word_tokenize(text)
    tokens = [token.lower() for token in tokens]
    tokens = [token for token in tokens if token not in stopwords.words('english')]
    lemmatizer = nltk.WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]
    return tokens

processed_articles = [preprocess(article) for article in articles]

# Sentiment Analysis
sentiments = [TextBlob(article).sentiment.polarity for article in articles]
average_sentiment = sum(sentiments) / len(sentiments)

# Topic Modeling
dictionary = Dictionary(processed_articles)
corpus = [dictionary.doc2bow(article) for article in processed_articles]
lda = LdaModel(corpus, num_topics=5, id2word=dictionary)

# Visualize Sentiment Scores
plt.figure(figsize=(10, 4))
plt.bar(articles.keys(), sentiments)
plt.xticks(rotation=90)
plt.ylabel('Sentiment Score')
plt.title('Sentiment Analysis of Articles')
plt.show()

import pyLDAvis.gensim_models
import joblib
joblib.parallel_backend('threading')

# Visualize Topics
lda_display = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary, sort_topics=False)
pyLDAvis.display(lda_display)
