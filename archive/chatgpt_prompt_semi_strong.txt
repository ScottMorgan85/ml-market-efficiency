I'd like to test semi-strong market efficiency of several different asset classes represented by bloomberg tickers. These are daily business day returns for 2007-04-03 to 2013-08-15 (except for the final column "Sentiment Score" - this is a normalized value to use in part 2 of this analysis). I have the dataframe loaded in jupyter labs using python, it is called "merged_df", below is additional information on it. I've also identified a number of events to test if these asset class are semi-strong efficient (see event table).

For part 1 create a python script to examine the returns for the asset class columns before, during and after the events list in the Event Table using the merged_df. Use the "Sentiment Score" to check the correlations during these periods as well. Create a simple table to start for each event and asset class.

merged_df.info():
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 4128 entries, 2007-04-03 to 2023-08-15
Data columns (total 9 columns):
 #   Column                      Non-Null Count  Dtype  
---  ------                      --------------  -----  
 0   US Large Cap Equities       4128 non-null   float64
 1   US Small Cap Equities       4128 non-null   float64
 2   US Invest.Grade Bonds       4128 non-null   float64
 3   US High Yield Bonds         4128 non-null   float64
 4   US Bank Loans               4128 non-null   float64
 5   Developed Country Equities  4128 non-null   float64
 6   Emerging Market Equities    4128 non-null   float64
 7   Emerging Market Debt        4128 non-null   float64
 8   Sentiment Score             4128 non-null   float64
dtypes: float64(9)
memory usage: 451.5 KB

merged_df.head():
US Large Cap Equities	US Small Cap Equities	US Invest.Grade Bonds	US High Yield Bonds	US Bank Loans	Developed Country Equities	Emerging Market Equities	Emerging Market Debt	Sentiment Score
2007-04-03	0.009439	0.010719	-0.001063	0.000192	0.000138	0.009498	0.009675	0.000189	0.068360
2007-04-04	0.001218	-0.001142	0.000613	-0.000497	0.000046	0.006205	0.011023	0.001093	0.075096
2007-04-05	0.003387	0.003195	-0.001224	0.000418	0.000097	0.002351	0.003008	0.000189	0.079543
2007-04-09	0.000830	-0.002080	-0.003147	0.000749	0.000745	-0.003081	0.006956	-0.000520	0.106911
2007-04-10	0.002422	0.003534	0.001839	0.000683	0.000229	0.009714	0.003584	0.000391	0.061565


Event Table:

| Event                   | Event Date  | Start Date  | End Date    |
|-------------------------|-------------|-------------|-------------|
| Lehman Collapse         | 9/15/2008   | 9/8/2008    | 9/22/2008   |
| Russia-Crimea Annexation| 3/18/2014   | 3/11/2014   | 3/25/2014   |
| ECB QE Announcement     | 1/22/2015   | 1/15/2015   | 1/29/2015   |
| Brexit Vote             | 6/23/2016   | 6/16/2016   | 6/30/2016   |
| COVID-19 Pandemic       | 3/11/2020   | 3/4/2020    | 3/18/2020   |


==============================

Forget all that. Start with the below code which works great. Make the  significance tests, other metrics and visualizations based off of this code:

import pandas as pd
import numpy as np
from datetime import datetime

merged_df.index = pd.to_datetime(merged_df.index)

# Assuming merged_df is already loaded into your environment
# Event Table
events = [
    {"name": "Lehman Collapse", "event_date": "9/15/2008", "start_date": "9/8/2008", "end_date": "9/22/2008"},
    {"name": "Russia-Crimea Annexation", "event_date": "3/18/2014", "start_date": "3/11/2014", "end_date": "3/25/2014"},
    {"name": "ECB QE Announcement", "event_date": "1/22/2015", "start_date": "1/15/2015", "end_date": "1/29/2015"},
    {"name": "Brexit Vote", "event_date": "6/23/2016", "start_date": "6/16/2016", "end_date": "6/30/2016"},
    {"name": "COVID-19 Pandemic", "event_date": "3/11/2020", "start_date": "3/4/2020", "end_date": "3/18/2020"},
]

asset_classes = merged_df.columns[:-1]  # Excluding the sentiment score

results = []

for event in events:
    start = datetime.strptime(event['start_date'], "%m/%d/%Y")
    end = datetime.strptime(event['end_date'], "%m/%d/%Y")

    event_data = merged_df.loc[start:end]

    event_results = {
        "Event": event['name'],
        "Event Date": event['event_date'],
    }
    
    for asset in asset_classes:
        event_results[f"{asset} Mean Return"] = event_data[asset].mean()
        event_results[f"{asset} Correlation with Sentiment"] = event_data[asset].corr(event_data['Sentiment Score'])

    results.append(event_results)

result_df = pd.DataFrame(results)



==================================================

Here is my section on semi-strong form efficieny for the article so far. For the NLP part, incorporate a short summary on the Daily News Sentiment Index provided by the Federal Reserve Bank of San Francisco and why it's advantageous and more efficient to use an aggregated sentiment provided by a third party for very broad market assumpions versus collecting individual news articles myself, building an NLP model myself and then calculating sentiment. This is more appropriate for individual company research.

Current semi-strong efficiency section:
### Semi-Strong Form Efficiency and Market Reaction Time ###

The semi-strong form of market efficiency suggests that all publicly available information is already incorporated into asset prices. Thus, markets should adjust quickly to new information like earnings announcements, economic data releases, geopoliticals events, etc. Thus, an investor cannot achieve abnormal returns using this information.

To test semi-strong efficiency, we examine returns before, during and after significant world events between 2007 to 2023. The table below provides these events and are by no means exhaustive.

| Event                  | Event Date       | Start_Date | End_Date   |
|------------------------|------------|------------|------------|
| Lehman Collapse        | 2008-09-15 | 2008-09-10 | 2008-09-20 |
| Brexit Vote            | 2016-06-23 | 2016-06-18 | 2016-06-28 |
| COVID-19 Pandemic      | 2020-03-11 | 2020-03-06 | 2020-03-16 |
| ECB QE Announcement    | 2015-01-22 | 2015-01-17 | 2015-01-27 |
| Russia-Crimea Annexation | 2014-03-18 | 2014-03-13 | 2014-03-23 |

This methodology gives a window of 5 days before and after each event. Index returns are then calculated for the event windows (i.e. event-driven returns) and compared against returns outside these windows (i.e. 'normal' period returns) to see if there are any significant deviations.  If there is a stark contrast between the event-driven returns and the normal period returns, this could imply that the market is adjusting to new publicly available information, which would support the semi-strong hypothesis. However, if these returns are predictably in one direction post events, it might hint at inefficiencies. 

We also integrate elements of Natural Language Processing (NLP) to quantify the sentiment or importance of news or announcements. By comparing NLP-derived metrics with the actual market returns, you can further test the semi-strong hypothesis by observing how quickly and accurately the market responds to the "tone" or "intensity" of new information.

Daily News Sentiment Index Description 1:
The Daily News Sentiment Index is a high frequency measure of economic sentiment based on lexical analysis of economics-related news articles. The index is described in Buckman, Shapiro, Sudhof, and Wilson (2020) and based on the methodology developed in Shapiro, Sudhof, and Wilson (2020).

Shapiro, Sudhof, and Wilson (2020, hereafter SSW), construct sentiment scores for economics-related news articles from 24 major U.S. newspapers compiled by the news aggregator service Factiva. The newspapers cover all major regions of the country, including some with extensive national coverage such as the New York Times and the Washington Post. SSW use articles with at least 200 words where Factiva identified the article’s topic as “economics” and the country subject as “United States.” Combining publicly available lexicons with a news-specific lexicon constructed by the authors, the study develops a sentiment-scoring model tailored specifically for newspaper articles.

SSW aggregate the individual article scores into a daily time-series measure of news sentiment, relying on a statistical adjustment that accounts for changes over time in the composition of the sample across newspapers. The Daily News Sentiment Index is constructed as a trailing weighted-average of time series, with weights that decline geometrically with the length of time since article publication. The data here will be regularly updated at a weekly frequency.

Daily News Sentiment Index Description 2:
The Daily News Sentiment Index is a high frequency measure of economic sentiment based on lexical analysis of economics-related news articles. The index is described in Buckman, Shapiro, Sudhof, and Wilson (2020) and based on the methodology developed in Shapiro, Sudhof, and Wilson (2020).

Shapiro, Sudhof, and Wilson (2020, hereafter SSW), construct sentiment scores for economics-related news articles from 24 major U.S. newspapers compiled by the news aggregator service Factiva. The newspapers cover all major regions of the country, including some with extensive national coverage such as the New York Times and the Washington Post. SSW use articles with at least 200 words where Factiva identified the article’s topic as “economics” and the country subject as “United States.” Combining publicly available lexicons with a news-specific lexicon constructed by the authors, the study develops a sentiment-scoring model tailored specifically for newspaper articles.

SSW aggregate the individual article scores into a daily time-series measure of news sentiment, relying on a statistical adjustment that accounts for changes over time in the composition of the sample across newspapers. The Daily News Sentiment Index is constructed as a trailing weighted-average of time series, with weights that decline geometrically with the length of time since article publication. The data here will be regularly updated at a weekly frequency.

