{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e191604-c29e-4a4b-9fd9-0e818e05da0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Image\n",
    "os.path.join(os.path.expandvars(\"%userprofile%\"),\"Documents\\\\Github\\\\ml-market-efficiency\\\\\")\n",
    "Image(filename=\"./images/title_pic_medium.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090bdb2d-2241-4942-8d37-5ecac340d28d",
   "metadata": {},
   "source": [
    "# Quantifying Market Efficiency: A Python & Machine Learning Approach Across Asset Classes #\n",
    "\n",
    "#### By Scott Morgan\n",
    "\n",
    "\n",
    "<Something about highlightlighting Bloomberg API, the package. Not to put down BQUANT>\n",
    "\n",
    "In the vast expanse of the financial universe, every asset class pulses with its own unique heartbeat, charting stories molded by economic dynamics, investor sentiments, geopolitical influences, and a plethora of factors that dictate global finance. From the frenetic pace of US equities, to the steady income of corporate bonds and the vast landscapes of foregin stock markets - each weaves tales of synchronicity, divergence, opportunity, and chaos.\n",
    "\n",
    "For the majority of my career, I have worked closely with portfolio managers and analysts on both the fixed income and equity sides of the isle. Conversations in research meetings, risk management reviews and marketing pitches consistently revolved around trading relative value within a firm's capital structure to seize alpha. It was a dance of navigating from high yield bonds at the top, right down to the equity, suggesting potential inefficiencies along the way. But where was the quantitative proof? Over time, this lingering curiosity has grown into a broader quest: Do market efficiencies truly differ across major asset classes? And if so, could machine learning help us unearth, quantify, and then exploit these inefficiencies?\n",
    "\n",
    "While the corridors of academia are lined with theses on this subject, my aim is to combine the analytical prowess of machine learning with my industry insights to unravel this enigma. This will not be a perfect study - I take assumptions, there will be oversights, but this is a starting point rather than a conclusive statement. Feedback, as always, is a gift.\n",
    "\n",
    "In the following sections we delve into historical return profiles, intricacies of market efficiency, its layers, and its significance in the sphere of active management. Leveraging various python libraries and the Bloomberg Professional Service, we'll dissect asset classes, zero in on company-specific instances, and navigate the potentials of machine learning-driven investment strategies. \n",
    "\n",
    "Remember that these views are personal, and as history has often demonstrated, past performance seldom guarantees future results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f763929d-07a7-443f-9007-9410d32fb6b8",
   "metadata": {},
   "source": [
    "## The Layers of Market Efficiency:\n",
    "The Efficient Market Hypothesis (EMH) has long been a subject of debate and analysis. Among the most notable contributions is Burton Malkiel's work, particularly his proposition of the stock market as a 'random walk', suggesting that future stock prices are independent of past prices.\n",
    "\n",
    "- **Weak Form:**  Past price and volume information cannot predict future prices. A key metric to gauge this is auto-correlation.\n",
    "\n",
    "- **Semi-Strong Form:** All publicly available information is reflected in asset prices. Key metrics include reaction times to earnings announcements, macroeconomic news, and other publicly available data sources.\n",
    "\n",
    "- **Strong Form:** All information, public and private, is reflected in asset prices. Metrics here would involve scrutinizing insider trading data and major institutional transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9a0627-4119-499a-b067-9b405e3101d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\blpapi\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\anaconda3\\envs\\blpapi\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\ProgramData\\anaconda3\\envs\\blpapi\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US Large Cap Equities', 'US Small Cap Equities', 'US Investment Grade Bonds', 'US High Yield Bonds', 'US Bank Loans', 'Developed Country Equities', 'Emerging Market Equities', 'Emerging Market Debt']\n",
      "Data loaded from existing CSV files.\n",
      "Data import process completed.\n",
      "Feature engineering completed.\n"
     ]
    }
   ],
   "source": [
    "from utils.helpers import *\n",
    "from utils.data_import_and_processing import *\n",
    "from utils.visualization import *\n",
    "from utils.market_efficiency_tests import plot_autocorrelation\n",
    "\n",
    "index_prices, index_returns = fetch_and_save_data(tickers, start_date, end_date, index_prices_path, index_returns_path)\n",
    "lagged_targets = generate_lagged_returns_and_targets(index_returns)\n",
    "generate_and_combine(index_prices,lagged_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed291caa-e136-4525-8665-0bddc9224990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run utils/market_efficiency_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a8e3b-27a2-4353-ae20-9efd2573b8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(days_after_event)\n",
    "print(asset_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd560e-0c7c-4730-989f-2c8e4924a54f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "422ceb12-ff1e-406c-98a8-ef90c70841c2",
   "metadata": {},
   "source": [
    "## The Landscape of Asset Returns ##\n",
    "Before delving into the intricacies of market efficiency, it's essential to set a foundation by understanding the landscape of asset returns over approximately 20 years of daily data. Delving into the return distributions of these asset classes, we see distinct patterns hinting at potential inefficiencies.\n",
    "\n",
    "For starters, the kurtosis value for **US Bank Loans** of 93.23 is staggering, indicating extreme fluctuations in returns, far more than what a normal distribution might suggest. Coupled with its extremely negative skewness, it points towards pronounced left-tailed events, suggesting the asset class might be prone to significant downturns. **US High Yield Bonds and Emerging Market Debt** are not far behind, with their substantial kurtosis values of 26.07 and 27.62 respectively, and pronounced negative skews, implying potential downside risks. On the contrast, asset classes like **US Large Cap Equities, US Small Cap Equities, and Emerging Market Equities** have milder skewness and kurtosis values and thus resemble a more normal distributions.\n",
    "\n",
    "However, every number has its story, and these statistics act as our preamble as we prepare to dive deeper into the realms of market efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3a32b-a41e-40c4-9a74-4917c9273bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, calculate skewness and kurtosis\n",
    "skew_kurt_df = calculate_skewness_kurtosis(index_returns)\n",
    "\n",
    "# Proceed to plot the distribution\n",
    "plot_distribution(index_returns, skew_kurt_df, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb6e08-a2e8-4101-bac4-38c9dd319a7c",
   "metadata": {},
   "source": [
    "## Unraveling Drawdown Dynamics: An Efficiency Paradox? ##\n",
    "Drawing insights from the largest drawdowns of various asset classes offers a nuanced perspective on market efficiency. **US High Yield Bonds and US Bank Loans**, for instance, display noticeable signs of potential inefficiency in their return distributions. Intriguingly, they also register the smallest drawdowns, which might seem counterintuitive at first. Smaller drawdowns often indicate a more stable asset class, but when combined with other inefficiencies, they raise questions. Could inefficiencies, in some cases, play a stabilizing role, or are there other underlying factors at play? These findings certainly set the stage for more detailed investigations.\n",
    "\n",
    "From an investment strategy standpoint, the stability of these assets, combined with their potential inefficiencies, makes them attractive candidates. It hints at possible opportunities to capitalize on mispricings while also enjoying a relatively stable performance. The following sections will delve deeper into designing investment strategies around these insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1658ef-e9d5-41a6-9cd9-56c7b0750634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_drawdowns(index_returns, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f45e8-f8de-409f-9296-acc508bd7b57",
   "metadata": {},
   "source": [
    "## Correlation Analsysis\n",
    "Peeling back another layer of the financial onion, correlation analysis offers insights into the relationship between asset class returns for a given time frame. With nearly 16 years of daily data to draw from, the co-movement patterns emerge with clarity. Most strikingly, **US Large Cap Equities and US Small Cap Equities** boast a high correlation of 0.917, illustrating their synchronous behavior in the financial markets. On the flip side, **US Investment Grade Bonds** show a mild negative correlation with both **Large and Small Cap Equities**, hinting at potential diversification benefits when constructing portfolios. A deeper dive into the correlation between **US High Yield Bonds and US Bank Loans** reveals a strong positive relationship, echoing a similar risk and return profile. Meanwhile, **Emerging Market Equities and Developed Country Equities** also move in tandem, as evidenced by their correlation coefficient of 0.782. \n",
    "\n",
    "Such correlation patterns are not only crucial for asset allocation decisions but also lay the groundwork for understanding the nuances of market efficiency and potential anomalies that might arise in interconnected markets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7236479-cbcd-442e-a3cd-3242f3b2a437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_correlation(index_returns, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a21d2d-ec44-479a-b3ce-bcfff0d5f323",
   "metadata": {},
   "source": [
    "## Empirical Analysis Across Asset Classes ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d01f8-5279-4906-9cfa-1ce5a9ca2546",
   "metadata": {},
   "source": [
    "### Weak Form Efficiency and Autocorrelation\n",
    "\n",
    "The weak form of the Efficient Market Hypothesis (EMH) posits that current asset prices fully incorporate all available historical information, primarily past prices. Hence, investors shouldn't be able to consistently earn above-average returns using historical data alone. In this context, if an asset class's returns display significant autocorrelation (i.e. if points are above the blue shaded regions in below graphs), it could indicate that past returns have some predictive power over future returns. This would challenge the notion of weak form efficiency, as it implies that there's some momentum or mean reversion in returns that could potentially be exploited\n",
    "\n",
    "#### Methodology\n",
    "\n",
    "The Autocorrelation Function (ACF) plots directly below reveal insights into the predictability and structure of financial time series, potentially hinting at market efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b672e05-9968-4f63-a060-4a08a22d53eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define index_returns, readable_names, and colors before this line\n",
    "plot_autocorrelation(index_returns, readable_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e6917-44cb-4267-8fb1-6858f927456d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Weak Form Findings\n",
    "Diving into the autocorrelation of asset class returns offers intriguing insights. **US Large Cap and Small Cap Equities** both show initial mean reversion tendencies with their pronounced negative autocorrelation at the first lag. **US Investment Grade Bonds** differ, hinting at potential momentum with positive initial lags. The momentum is also evident in **US High Yield Bonds and US Bank Loans**, where the persistence of positive autocorrelation across several lags suggests potential inefficiencies or return predictability. Meanwhile, **Developed Country Equities** and **Emerging Market Equities** present a mixed interplay of market dynamics, revealing potential inefficiencies in their patterns. **Emerging Market Debt**, much like the **High Yield Bonds**, also suggests momentum, especially in the initial lags. \n",
    "\n",
    "***These findings suggest that past returns of US High Yield Bonds, US Bank Loans and Emerging Market Debt during the time period potentially explain future returns; thus inefficiencies could exist in these markets. The remaining asset classes behave as a random walk and are potentially weakly efficient.***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725de1bc-d488-43d1-9d1b-674fb274df64",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Semi-Strong Form Efficiency and Using Machine Learning to Test Market Reaction ###\n",
    "\n",
    "The semi-strong form of market efficiency asserts that asset prices adjust rapidly to all publicly available information (green line below) and are unpredictable (i.e. a random walk), so neither technical analysis nor fundamental analysis can consistently achieve superior returns. If markets are semi-stong inefficient, prices will adjust gradually and should be more predictable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d59b5-56b3-4f81-b0cc-4921b499ca24",
   "metadata": {},
   "source": [
    "#### Methodology\n",
    "\n",
    "To examine the semi-strong form of the EMH, I used an XGBoost classifier—a very popular gradient boosting framework - because of its versatility and transparency. The essence of our test: If the models, trained on historical technical indicators, can consistently predict post-event returns with an accuracy significantly greater than 50%, it implies that the asset class doesn't rapidly incorporate event-related data, challenging the semi-strong form of EMH. Conversely, an accuracy close to or below 50% would suggest that post-event returns are seemingly random after accounting for known event information, supporting the semi-strong form.\n",
    "\n",
    "The dataset was split into 'event' and 'non-event' periods. Models were trained for each respective asset class (8 total) on the 'non-event' days and put to the test on the 'event' days, mimicking a real-world scenario where investors aim to exploit new public data. Below is an interactive summary of the periods. The 'event days' are a 30 day window following the announcement of a event (see reference table below chart for expect starts). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1c418-3fef-405e-a3d6-32d403dce935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming index_returns is already loaded\n",
    "plot_cumulative_returns_with_events(index_returns)\n",
    "display_event_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190bb305-40f8-4b87-a5f7-1097ba1a4647",
   "metadata": {},
   "source": [
    " I constructed a number of trailing technical features from the historical price data of the asset classes. These are the independent variables. For the targets, or dependent variables, I used the next day (t+1) returns of the respective indexes. Below are the descriptions of each.\n",
    " \n",
    "<u>**Dependent Variables**</u>\n",
    "- **SMA (Simple Moving Average):** The average price of an n asset over a specific number of days. It's \"simple\" because each day's price has equal weight.\n",
    "\n",
    "- **EMA (Exponential Moving Average):** Similar to SMA, but it gives more weight to recent prices. This responsiveness means EMA reacts more quickly to price changes than SMA.\n",
    "\n",
    "- **RSI (Relative Strength Index):** A momentum oscillator ranging from 0 to 100, indicating the speed and change of price movements. Typically, an RSI above 70 suggests an asset might be overbought, while an RSI below 30 suggests it might be oversold.\n",
    "\n",
    "- **MACD (Moving Average Convergence Divergence):** A trend-following momentum indicator, represented by the difference between a short-term EMA and a long-term EMA. MACD signals are typically derived from its crossovers with its signal line (an EMA of the MACD itself).\n",
    "\n",
    "<u>**Targets/Dependent Variables**</u>\n",
    "- **Next Day Returns:** The target variables are the next day (t+1) returns of the respective indexes. I chose to use future returns versus prices because predicting returns is more feasible; returns capture relative changes and inherent volatility, providing clearer insight into an asset's dynamic behavior. Next, I transformed the returns into binary variables: '1' for a positive return and '0' for a negative return. By converting continuous next-day returns into binary indicators, we focus on predicting directional trends rather than exact magnitudes, simplifying the modeling process and reducing noise.\n",
    "\n",
    "<u>**Evaluation**</u>\n",
    "\n",
    "We evaluate the models at T+5 days, T+30 days, T+60 days and T+90 days. By testing the model's performance across varying time periods after an event, I am examining the speed at which new information is incorporated into prices across different asset classes. If a model consistently generates accurate predictions greater than 50% long after an event, it could indicate inefficiencies in the market or that the market is taking longer to assimilate the new information, which would challenge the semi-strong form of the EMH.\n",
    "\n",
    "In evaluating the models across different time periods, the primary metric of interest remains the F1 Score. This metric becomes especially significant in contexts like financial trading where both types of errors – false positives (erroneously buying) and false negatives (missing out on a genuine buying opportunity) – are financially penalizing. The F1 Score serves as an optimal metric because it is the harmonic mean of precision and recall, ensuring that a model not only provides trustworthy positive predictions (signifying high precision) but also efficiently identifies the majority of genuine positive opportunities (indicating high recall). \n",
    "\n",
    "While we have multiple metrics at our disposal, the dynamic nature of financial markets necessitates the exploration of models across varied time frames. This is crucial as different periods might unveil diverse patterns or anomalies. By identifying the top-performing model across the most relevant time frame for our strong form tests, we're better positioned to harness potential market efficiencies or pinpoint areas of exploitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e2483-9865-4ead-a0b2-3eef1c6aff3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(days_after_event)\n",
    "print(asset_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca77c8a-d904-43a2-9689-72b82456e920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ffb5e-64c2-4436-be51-90c3b9477e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# technical_features = generate_technical_indicators(index_prices)\n",
    "# combined_data = pd.concat([technical_features, lagged_targets], axis=1)\n",
    "# combined_data.dropna(inplace=True) # Drop NaN value\n",
    "\n",
    "# # Event Data Preparation\n",
    "\n",
    "# data = {\n",
    "#     'Event': ['Lehman Collapse','ECB QE Announcement', 'Brexit Vote', 'COVID-19 Pandemic','Russia-Ukraine/Fed Hikes','SVB Collapse'],\n",
    "#     'Event Date': ['9/15/2008', '1/22/2015', '6/23/2016', '3/11/2020','2/25/2022','3/10/2023']\n",
    "# }\n",
    "\n",
    "# df_events = pd.DataFrame(data)\n",
    "# df_events['Event Date'] = pd.to_datetime(df_events['Event Date'])\n",
    "\n",
    "# def get_test_dates(df, pre_days, post_days_list):\n",
    "#     test_dates_dict = {}\n",
    "#     for post_days in post_days_list:\n",
    "#         test_dates = []\n",
    "#         for date in df['Event Date']:\n",
    "#             start_date = date - timedelta(days=pre_days)\n",
    "#             end_date = date + timedelta(days=post_days)\n",
    "#             test_dates.extend(pd.date_range(start=start_date, end=end_date).tolist())\n",
    "#         test_dates_dict[post_days] = test_dates\n",
    "#     return test_dates_dict\n",
    "\n",
    "\n",
    "# List of asset classes (you might need to adjust this according to your dataset)\n",
    "# asset_classes = [\n",
    "#     'US Large Cap Equities', \n",
    "#     'US Small Cap Equities', \n",
    "#     'US Investment Grade Bonds', \n",
    "#     'US High Yield Bonds', \n",
    "#     'US Bank Loans', \n",
    "#     'Developed Country Equities', \n",
    "#     'Emerging Market Equities', \n",
    "#     'Emerging Market Debt'\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Get test dates for all intervals\n",
    "test_dates_dict = get_test_dates(df_events, 0, days_after_event)\n",
    "\n",
    "# Dictionary to store evaluation metrics for each day interval\n",
    "all_evaluation_metrics = {}\n",
    "\n",
    "# Ensure the models directory exists\n",
    "if not os.path.exists('./models/'):\n",
    "    os.makedirs('./models/')\n",
    "\n",
    "X_train_dict = {}\n",
    "y_train_dict = {}\n",
    "X_test_dict = {}\n",
    "y_test_dict = {}    \n",
    "\n",
    "def train_initial_models(X_train_dict, y_train_dict, X_test_dict, y_test_dict):\n",
    "    print(\"Training initial models...\")\n",
    "    \n",
    "    # DataFrame to store feature importances for all assets\n",
    "    feature_importances_df = pd.DataFrame()\n",
    "    \n",
    "    results = {}\n",
    "    for asset in X_train_dict:\n",
    "        X_train = X_train_dict[asset]\n",
    "        y_train = y_train_dict[asset]\n",
    "        \n",
    "        X_test = X_test_dict[asset]\n",
    "        y_test = y_test_dict[asset]\n",
    "        \n",
    "        model = XGBClassifier(objective='binary:logistic', tree_method='gpu_hist')\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Extract feature importances\n",
    "        feature_importances = model.feature_importances_\n",
    "        temp_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': feature_importances,\n",
    "            'Asset': asset  # Adding the asset name to differentiate\n",
    "        })\n",
    "        # feature_importances_df = feature_importances_df.append(temp_df)\n",
    "        feature_importances_df = pd.concat([feature_importances_df, temp_df], ignore_index=True)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        results[asset] = {\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'F1 Score': f1_score(y_test, y_pred),\n",
    "            'ROC AUC': roc_auc_score(y_test, y_pred_proba)\n",
    "        }\n",
    "\n",
    "\n",
    "#Loop through each day interval\n",
    "for days in days_after_event:\n",
    "    test_dates = test_dates_dict[days]\n",
    "    for asset_class in asset_classes:\n",
    "        \n",
    "        # Filter columns related to the current asset class\n",
    "        relevant_columns = [col for col in combined_data.columns if col.startswith(asset_class)]\n",
    "        \n",
    "        asset_data = combined_data[relevant_columns]\n",
    "        \n",
    "        # Assuming the target variable for each asset class is simply its name (e.g., 'US Large Cap Equities')\n",
    "        if asset_class not in asset_data.columns:\n",
    "            continue\n",
    "        \n",
    "        train = asset_data[~asset_data.index.isin(test_dates)]\n",
    "        test = asset_data[asset_data.index.isin(test_dates)]\n",
    "\n",
    "        X_train_dict[asset_class] = train.drop(asset_class, axis=1)\n",
    "        y_train_dict[asset_class] = train[asset_class]\n",
    "        \n",
    "        X_test_dict[asset_class] = test.drop(asset_class, axis=1)\n",
    "        y_test_dict[asset_class] = test[asset_class]\n",
    "\n",
    "        # Step 2: Train the models using the dictionaries\n",
    "        evaluation_metrics, model = train_initial_models(X_train_dict, y_train_dict, X_test_dict, y_test_dict)\n",
    "        all_evaluation_metrics[f\"{asset_class}_{days} days\"] = evaluation_metrics\n",
    "\n",
    "        # Save the trained model\n",
    "        with open(f'./models/{asset_class}_model_{days}_days.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "# Aggregate all evaluation metrics into a single dataframe\n",
    "staging_evaluation_df = pd.concat({k: pd.DataFrame(v).T for k, v in all_evaluation_metrics.items()}, axis=0)\n",
    "\n",
    "# Use the unstack method to pivot the DataFrame\n",
    "pivot_df = staging_evaluation_df.unstack(level=0)\n",
    "\n",
    "# The above line will pivot the 'Time Period' level (which appears to be level 0) to columns\n",
    "\n",
    "# If you want to remove the top-level column (F1 Score) to have a cleaner DataFrame, you can do:\n",
    "pivot_df.columns = pivot_df.columns.get_level_values(1)\n",
    "\n",
    "# Sort the columns for better organization (if needed)\n",
    "ordered_columns = ['5 days', '30 days', '60 days', '90 days']\n",
    "final_evaluation_df = pivot_df[ordered_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a36bfa5-acf1-439a-8e56-d136c15a7f1e",
   "metadata": {},
   "source": [
    "#### Semi-Strong Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba945b3-5727-4296-ad83-0c4cd64aeded",
   "metadata": {},
   "source": [
    "The results suggest a nuanced understanding of market efficiency across different time intervals. \n",
    "\n",
    "- For US Large Cap Equities, the F1 scores progressively increased from 0.64 over 5 days to 0.56 over 90 days. While this indicates a trend towards randomness and supports the semi-strong form of the Efficient Market Hypothesis (EMH), there are fluctuations that warrant closer examination. On the other hand, US Small Cap Equities exhibited scores ranging from 0.45 to 0.52, showcasing more pronounced volatility in shorter time frames, which stabilizes over longer durations.\n",
    "\n",
    "- US Investment Grade Bonds, with F1 scores ranging from 0.43 to 0.55, exhibit moderate efficiencies over the tested time intervals. However, **US High Yield Bonds and US Bank Loans, displaying scores like 0.62 and 0.75, respectively, over a 90-day period, reinforce the earlier observation of stronger pattern recognition capabilities. These scores, being considerably above the 0.5 mark, suggest potential inefficiencies in these markets. It implies that there's room for investors or traders to potentially capitalize on certain patterns or information.**\n",
    "\n",
    "- Developed Country Equities, Emerging Market Equities, and Emerging Market Debt display scores that hover around the 0.5 to 0.6 range, suggesting a degree of randomness but with some hints of pattern recognition, especially in the longer time frames. This could be indicative of semi-strong market efficiency, but the door remains open for further analysis and scrutiny.\n",
    "\n",
    "To summarize, while some asset classes seem to align more closely with the semi-strong form of the EMH, others, especially US High Yield Bonds and US Bank Loans, indicate potential market inefficiencies that might be exploitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ba388-cd80-47a5-a814-a03face40a78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def highlight_f1(row):\n",
    "    return ['background-color: yellow' if col == 'F1 Score' else '' for col in row.index]\n",
    "\n",
    "styled_evaluation_df = (final_evaluation_df.style\n",
    "                        .apply(highlight_f1)\n",
    "                        .format(\"{:.2f}\")\n",
    "                        .set_caption(\"<b style='font-size: 16px'>F1 Metrics for XGBoost Across Different Time Intervals</b>\")\n",
    "                        .set_table_styles({\n",
    "                            'F1 Score': [{'selector': '',\n",
    "                                          'props': [('color', 'black'),\n",
    "                                                    ('font-weight', 'bold')]}]\n",
    "                       \n",
    "                        }))\n",
    "display(styled_evaluation_df)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming you have your final_evaluation_df DataFrame defined\n",
    "\n",
    "# Set a seaborn style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define colors for each category\n",
    "colors = sns.color_palette(\"husl\", n_colors=len(final_evaluation_df))\n",
    "\n",
    "# Iterate through each row (category) in final_evaluation_df\n",
    "for i, (index, row) in enumerate(final_evaluation_df.iterrows()):\n",
    "    # Plot a smooth line for each category with the corresponding color\n",
    "    plt.plot(row.index[1:], row.values[1:], marker='o', label=index, color=colors[i], linewidth=2)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"F1 Score Across Different Time Intervals\", fontsize=16)\n",
    "plt.xlabel(\"Time Intervals\", fontsize=14)\n",
    "plt.ylabel(\"F1 Score\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f354f5-817a-4d94-bf1d-64d80a3322ee",
   "metadata": {},
   "source": [
    "A few caveats to bear in mind:\n",
    "\n",
    "- **1** These results do not provide definitive proof regarding the EMH but rather offer insights into varying efficiency levels across asset classes.\n",
    "\n",
    "- **2** The models weren't subjected to hyperparameter tuning, which could potentially enhance their performance and provideclearer insights.\n",
    " \n",
    "Next, I will leverage transfer learning to test strong form efficiency, capitalizing on the top performing model (in this case, US Bank Loans, given its high F1 score) and applying it to other assets. This would test the hypothesis of whether patterns recognized in one asset class can be translated and exploited in others, further challenging the tenets of the EMH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3352c9c-a82b-47f5-bc11-46ce05a3c1db",
   "metadata": {},
   "source": [
    "### Strong Form Efficiency & Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49674e39-1819-4d7d-844c-23473dceed99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(filename=\"./images/semi_strong_visual.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df6388-23aa-48a0-9bb7-63afc58cecab",
   "metadata": {},
   "source": [
    "The strong form of market efficiency posits that all information, public and private, is instantly reflected in asset prices. Testing this hypothesis requires examining whether models trained on one set of data (say, public data) can generalize and make predictions on another set (potentially mimicking private information scenarios). Transfer learning is a natural fit for this kind of evaluation.\n",
    "<!-- \n",
    "<center>\n",
    "<img src=\"attachment:f4fc5060-33aa-4984-a10c-2d2d1a5f82c9.png\" style=\"width: 600px\">\n",
    "</center> -->\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a31bca-ae3d-498e-bb2c-de240cf62bf9",
   "metadata": {},
   "source": [
    "#### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ced7bd-7751-4833-b385-66cc489e948e",
   "metadata": {},
   "source": [
    "Historically a powerhouse technique in deep learning, especially in the domain of image and language processing, transfer learning allows models trained on one dataset to apply their knowledge to a different, though related, dataset. In our context, this means leveraging the patterns and intricacies learned from one asset class, such as bank loans, to potentially enhance the model's predictive capability on another, like equities or bonds. The underlying assumption is that there might be hidden, overarching market dynamics or patterns that are universal across different asset classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b694a-e26a-436b-a93f-0b86d7ec250f",
   "metadata": {},
   "source": [
    "The choice of using transfer learning is multifaceted. Not only does it provide an opportunity to harness the potential cross-asset predictive power, but it also challenges the very core of the strong form market efficiency. If our models, armed with transfer learning, can consistently outperform the market, it might suggest that there are inefficiencies, or at least patterns, that even sophisticated market participants haven't fully grasped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79134ac-15ed-445a-b2df-1bce7bfb2702",
   "metadata": {},
   "source": [
    "I initially turned to the XGBoost algorithm, famed for its precision and adaptability. Yet, the intricate temporal dynamics and complex non-linearities inherent in financial data exposed its limitations. Financial markets, with their multifaceted interactions and time-sensitive structures, demand models that can capture depth and complexity. Enter Keras deep learning models. These models, equipped with capabilities to automatically discern crucial features, adeptly handle sequential data, and intricately model inter-variable relationships, emerged as a more suitable choice. This shift from XGBoost to a Keras-based approach underscores the criticality of aligning the modeling tool with the complexities of the task, especially in the enigmatic realm of finance. \n",
    "\n",
    "So in order to complete this series of tests, I had to retrain our XGBoost models using Keras. The results are below and are similar to our original results, with US Leveraged Loans outperforming and will therefore be used as our base model. Transfer learning comes into play when we adapt the US Leveraged Loan model to other asset classes. If this adapted model also performs well on other asset classes, it might suggest that certain information is consistently not being incorporated across multiple asset classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd85f2-25ec-44ff-84cd-5cace1a31a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "# Assuming the models are saved in the './models/' directory\n",
    "base_model_path = './models/US Bank Loans_model.pkl'\n",
    "base_model = joblib.load(base_model_path)\n",
    "\n",
    "def evaluate_on_base_model(base_model, X_test_dict, y_test_dict):\n",
    "    evaluation_results = {}\n",
    "    feature_importances_df = pd.DataFrame()  # DataFrame to store feature importances\n",
    "\n",
    "    for asset, X_test in X_test_dict.items():\n",
    "        # Skip the base model's asset class itself\n",
    "        if asset == \"US Bank Loans\":\n",
    "            continue\n",
    "\n",
    "        y_test = y_test_dict[asset]\n",
    "\n",
    "        # Standardize variable names to fit base model input\n",
    "        rename_dict = {col: col.replace(asset, \"US Bank Loans\") for col in X_test.columns}\n",
    "        X_test_renamed = X_test.rename(columns=rename_dict)\n",
    "\n",
    "        y_pred = base_model.predict(X_test_renamed)\n",
    "        y_pred_proba = base_model.predict_proba(X_test_renamed)[:, 1]\n",
    "\n",
    "        evaluation_results[asset] = {\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred, zero_division=1),  # This sets precision to 1 when there are no predicted positives.\n",
    "            'F1 Score': f1_score(y_test, y_pred),\n",
    "            'ROC AUC': roc_auc_score(y_test, y_pred_proba)\n",
    "        }\n",
    "\n",
    "        # Extract feature importances for the asset and store in the DataFrame\n",
    "        temp_df = pd.DataFrame({\n",
    "            'Feature': X_test_renamed.columns,\n",
    "            'Importance': base_model.feature_importances_,\n",
    "            'Asset': asset  # Adding the asset name for differentiation\n",
    "        })\n",
    "        feature_importances_df = pd.concat([feature_importances_df, temp_df], ignore_index=True)\n",
    "\n",
    "    return evaluation_results, feature_importances_df\n",
    "\n",
    "transfer_learning_results, importances_df = evaluate_on_base_model(base_model, X_test_dict, y_test_dict)\n",
    "\n",
    "# Convert results dictionary into a DataFrame\n",
    "transfer_learning_df = pd.DataFrame(transfer_learning_results).T\n",
    "\n",
    "# Display the Evaluation Results DataFrame\n",
    "print(transfer_learning_df)\n",
    "\n",
    "# Display the Feature Importances DataFrame\n",
    "print(importances_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8dab7-83a2-4283-8936-a28427c1ae34",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "US High Yield Bonds has the highest accuracy, F1 score, and ROC AUC among all asset classes. This suggests that the model performs best when predicting this asset class. All the asset classes have Accuracy and ROC AUC scores slightly above 0.5 or below, indicating the models are not performing much better than a random guess. The Precision is just above 0.5 for most asset classes, meaning slightly more than half of the positive predictions are actually correct. The F1 scores are slightly higher than the accuracy in most cases, suggesting that the harmonic balance between precision and recall is somewhat better than raw accuracy for these predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26daec0-f050-45e7-867b-2326524a43c0",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "If we look across all asset classes, \"US Bank Loans_RSI_14\" consistently has the highest importance score (~0.213). This suggests that the RSI_14 (Relative Strength Index for 14 days) of US Bank Loans is the most influential feature in predicting the given asset classes.\n",
    "Other features like \"US Bank Loans_SMA_30\", \"US Bank Loans_EMA_60\", etc., have importance scores that vary but are generally lower than RSI_14. This means they are less influential in the model's predictions compared to RSI_14. The importance values for the same features are consistent across different asset classes. This suggests that the model sees the same relative importances for these features regardless of which asset class it's predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efffba-c57f-48fd-91bd-042bb2c496c0",
   "metadata": {},
   "source": [
    "The model's predictive performance across asset classes is mediocre at best, given the scores are not significantly better than random guessing. Among the features, RSI_14 is the most important for making predictions. Further refinement and perhaps feature engineering might be necessary to improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf0d3a-1410-45f0-9ebe-eebe27c219b4",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f489a-0b07-4ea5-933f-203ec369b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-gpu==2.5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd3b72d-fb0f-4add-a4a1-0116fedab40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e768d30-33b9-4605-951d-1c014cf41f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d891d3-3670-4975-a5ce-a263f228336e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import h5py\n",
    "\n",
    "def generate_lagged_returns_and_targets(df):\n",
    "   \n",
    "    lagged_returns = index_returns.shift(-1)\n",
    "    lagged_returns.dropna(inplace=True)\n",
    "    lagged_returns.columns = [f\"{col}\" for col in lagged_returns.columns]\n",
    "    \n",
    "    for column in lagged_returns.columns:\n",
    "        lagged_returns[column] = (lagged_returns[column] > 0).astype(int)\n",
    "    \n",
    "    return lagged_returns\n",
    "\n",
    "lagged_targets = generate_lagged_returns_and_targets(index_returns)\n",
    "\n",
    "\n",
    "\n",
    "def MACD(series, short_window, long_window, signal_window):\n",
    "    short_ema = series.ewm(span=short_window, adjust=False).mean()\n",
    "    long_ema = series.ewm(span=long_window, adjust=False).mean()\n",
    "    macd_line = short_ema - long_ema\n",
    "    signal_line = macd_line.rolling(window=signal_window).mean()\n",
    "    return macd_line, signal_line\n",
    "\n",
    "def generate_technical_indicators(df):\n",
    "    indicator_dataframes = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        # Skip 'Sentiment Score' for now as you instructed\n",
    "        if column == 'Sentiment Score':\n",
    "            continue\n",
    "\n",
    "        sma_30 = ta.trend.SMAIndicator(close=df[column], window=30).sma_indicator()\n",
    "        sma_60 = ta.trend.SMAIndicator(close=df[column], window=60).sma_indicator()\n",
    "\n",
    "        ema_30 = ta.trend.EMAIndicator(close=df[column], window=30).ema_indicator()\n",
    "        ema_60 = ta.trend.EMAIndicator(close=df[column], window=60).ema_indicator()\n",
    "\n",
    "\n",
    "        rsi_14 = ta.momentum.RSIIndicator(close=df[column], window=14).rsi()\n",
    "        macd, macdsignal = MACD(df[column], 12, 26, 9)  # modified the unpacking\n",
    "\n",
    "        indicators = pd.concat([\n",
    "            sma_30.rename(f\"{column}_SMA_30\"),\n",
    "            sma_60.rename(f\"{column}_SMA_60\"),\n",
    "            ema_30.rename(f\"{column}_EMA_30\"),\n",
    "            ema_60.rename(f\"{column}_EMA_60\"),\n",
    "            rsi_14.rename(f\"{column}_RSI_14\"),\n",
    "            macd.rename(f\"{column}_MACD\"),\n",
    "            macdsignal.rename(f\"{column}_MACD_Signal\")\n",
    "        ], axis=1)\n",
    "\n",
    "        indicator_dataframes.append(indicators)\n",
    "\n",
    "    combined_indicators = pd.concat(indicator_dataframes, axis=1)\n",
    "    combined_indicators = combined_indicators.dropna()\n",
    "\n",
    "    return combined_indicators\n",
    "\n",
    "technical_features = generate_technical_indicators(index_prices)\n",
    "combined_data = pd.concat([technical_features, lagged_targets], axis=1)\n",
    "combined_data.dropna(inplace=True)\n",
    "\n",
    "data = {\n",
    "    'Event': ['Lehman Collapse','ECB QE Announcement', 'Brexit Vote', 'COVID-19 Pandemic','Russia-Ukraine/Fed Hikes','SVB Collapse'],\n",
    "    'Event Date': ['9/15/2008', '1/22/2015', '6/23/2016', '3/11/2020','2/25/2022','3/10/2023']\n",
    "}\n",
    "\n",
    "df_events = pd.DataFrame(data)\n",
    "df_events['Event Date'] = pd.to_datetime(df_events['Event Date'])\n",
    "\n",
    "def get_test_dates(df, pre_days, post_days):\n",
    "    test_dates = []\n",
    "    for date in df['Event Date']:\n",
    "        start_date = date - timedelta(days=pre_days)\n",
    "        end_date = date + timedelta(days=post_days)\n",
    "        test_dates.extend(pd.date_range(start=start_date, end=end_date).tolist())\n",
    "    return test_dates\n",
    "\n",
    "test_dates = get_test_dates(df_events, 0, 100)\n",
    "combined_data = combined_data.drop(combined_data.filter(regex='Test').columns, axis=1)\n",
    "\n",
    "train = combined_data[~combined_data.index.isin(test_dates)]\n",
    "test = combined_data[combined_data.index.isin(test_dates)]\n",
    "\n",
    "def prepare_data_for_ml(dataframe):\n",
    "    print(\"Data preparation started...\")\n",
    "    \n",
    "    asset_columns = ['US Large Cap Equities', 'US Small Cap Equities',\n",
    "                     'US Investment Grade Bonds', 'US High Yield Bonds', 'US Bank Loans',\n",
    "                     'Developed Country Equities', 'Emerging Market Equities',\n",
    "                     'Emerging Market Debt']\n",
    "    \n",
    "    X_dict, y_dict = {}, {}\n",
    "    \n",
    "    for asset_name in asset_columns:\n",
    "        feature_columns = [ \n",
    "            f\"{asset_name}_SMA_30\",\n",
    "            f\"{asset_name}_SMA_60\",\n",
    "            f\"{asset_name}_EMA_30\",\n",
    "            f\"{asset_name}_EMA_60\",\n",
    "            f\"{asset_name}_RSI_14\",\n",
    "            f\"{asset_name}_MACD\",\n",
    "            f\"{asset_name}_MACD_Signal\"]\n",
    "\n",
    "        X_dict[asset_name] = dataframe[feature_columns]\n",
    "        y_dict[asset_name] = dataframe[asset_name]\n",
    "    \n",
    "    print(\"Data preparation completed!\")\n",
    "    return X_dict, y_dict\n",
    "\n",
    "X_train_dict, y_train_dict = prepare_data_for_ml(train)\n",
    "X_test_dict, y_test_dict = prepare_data_for_ml(test)\n",
    "\n",
    "def train_initial_models(X_train_dict, y_train_dict, X_test_dict, y_test_dict):\n",
    "    print(\"Training initial models...\")\n",
    "    \n",
    "    results = {}\n",
    "    for asset in X_train_dict:\n",
    "        X_train = X_train_dict[asset]\n",
    "        y_train = y_train_dict[asset]\n",
    "        \n",
    "        X_test = X_test_dict[asset]\n",
    "        y_test = y_test_dict[asset]\n",
    "        \n",
    "        # Standardizing the data\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "                \n",
    "        # Define the Keras model\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_split=0.2)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_proba = model.predict(X_test).flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        results[asset] = {\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'F1 Score': f1_score(y_test, y_pred),\n",
    "            'ROC AUC': roc_auc_score(y_test, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        # Save each model to the models folder\n",
    "        # model.save(f'./models/{asset}_model.h5')\n",
    "        model.save(f'./models/{asset}_model.keras')\n",
    "    \n",
    "    print(\"Initial models trained!\")\n",
    "    return results\n",
    "\n",
    "evaluation_metrics = train_initial_models(X_train_dict, y_train_dict, X_test_dict, y_test_dict)\n",
    "\n",
    "# Convert the evaluation_metrics dictionary into a DataFrame\n",
    "evaluation_df = pd.DataFrame(evaluation_metrics).T\n",
    "\n",
    "# Display the DataFrame\n",
    "print(evaluation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ee594-a77c-4392-a94a-0124d2a09b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# 1. Load the Pre-trained Model\n",
    "base_model = load_model('./models/US Bank Loans_model.keras')\n",
    "\n",
    "# 2. Fine-tuning (Optional)\n",
    "# Here's an example of freezing the first two layers\n",
    "for layer in base_model.layers[:2]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# A function to adapt and evaluate models using transfer learning\n",
    "def transfer_and_evaluate(asset):\n",
    "    X_train = X_train_dict[asset]\n",
    "    y_train = y_train_dict[asset]\n",
    "    X_test = X_test_dict[asset]\n",
    "    y_test = y_test_dict[asset]\n",
    "    \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Fine-tuning\n",
    "    model = Sequential(base_model.layers[:-1])  # Exclude last layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_split=0.2)\n",
    "    \n",
    "    y_pred_proba = model.predict(X_test).flatten()\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    asset_results = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    return asset_results\n",
    "\n",
    "# 3. Evaluate on New Data\n",
    "transfer_results = {}\n",
    "asset_columns = ['US Large Cap Equities', 'US Small Cap Equities',\n",
    "                 'US Investment Grade Bonds', 'US High Yield Bonds', \n",
    "                 'Developed Country Equities', 'Emerging Market Equities',\n",
    "                 'Emerging Market Debt']\n",
    "\n",
    "for asset in asset_columns:\n",
    "    transfer_results[asset] = transfer_and_evaluate(asset)\n",
    "\n",
    "transfer_evaluation_df = pd.DataFrame(transfer_results).T\n",
    "print(transfer_evaluation_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea521e-6165-4b09-a1f1-684b3d0cc17a",
   "metadata": {},
   "source": [
    "Given that some bank loan companies are private and might possess material non-public information, the use of data from this asset class could mean the model is tapping into information patterns not fully available or understood by the broader market. If transfer learning models consistently and significantly outperform the market by leveraging this data, it would suggest the potential presence of inefficiencies in the market's pricing of public information, even if not a direct contradiction of the strong form of the EMH. However, it's essential to approach such conclusions with caution. Above-average performance might be due to overfitting, specific market conditions, or other nuances. Furthermore, while back-tested results can be promising, real-world trading, with its associated costs and complexities, can present a different picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b0544-acf5-4410-b143-a6dd36dfe9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e59dc-6a13-440b-8a36-5f8a78a93317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c63177d2-9da1-4bb2-b421-c170f7c1c998",
   "metadata": {},
   "source": [
    "## Company-specific Instances\n",
    "Zoom into specific examples to showcase discrepancies or efficiencies within an individual company's capital structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e0dd8-048c-433e-9f0c-6f4993f90031",
   "metadata": {},
   "source": [
    "When drilling down, it's essential to select companies that have both actively traded equities and bonds. Here are some suggestions:\n",
    "\n",
    "***Apple Inc.***: Known for issuing bonds despite having a huge cash reserve. Its equity and bonds could respond differently to product launches or other news.\n",
    "\n",
    "***Microsoft Corporation***: Another tech giant with both active equities and bonds.\n",
    "\n",
    "***AT&T Inc.***: A company with a significant debt load, providing opportunities to analyze both equity and debt.\n",
    "\n",
    "***Amazon Inc.***: Given its dominance and wide array of business activities, news and events could trigger varied responses in its capital structure.\n",
    "\n",
    "***Walmart Inc.***: A retail giant, its equity and debt dynamics could be influenced by economic cycles, consumer behavior, etc.\n",
    "\n",
    "Once you've chosen a company and run the initial code, please share the results for interpretation, and I'll assist you in deriving insights from the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf62bbc-9fe8-4254-935d-c309da82a26f",
   "metadata": {},
   "source": [
    "## ML-Driven Investment Strategies\n",
    "Introduce and discuss potential machine learning strategies that could exploit detected inefficiencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf535875-b2b0-42a5-854e-7889873a1479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ad0ae-b198-4594-a6f1-33c6d45079c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aec5e6ef-04c9-4240-9d09-1d1ff4017953",
   "metadata": {},
   "source": [
    "## Limitations, Assumptions and Future Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871e141-89c7-475c-91cd-fcad04776af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "only for time period, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98f8e6-e3eb-4375-af74-34723a12f4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7ce6e59-1da1-44f1-8125-1d5c13fb362c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Recap the findings, discuss their implications, and leave the readers with thoughts on future possibilities in the realm of quantitative finance and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077cd0c2-879b-4940-be0d-47984d786a93",
   "metadata": {},
   "source": [
    "## References \n",
    "Malkiel, B. G. (2007, December 17). A Random Walk Down Wall Street: The Time-Tested Strategy for Successful Investing (Ninth Edition). W. W. Norton & Company.\n",
    "\n",
    "Shapiro, Adam Hale, Moritz Sudhof, and Daniel Wilson. 2020. \"Measuring News Sentiment,\" Federal Reserve Bank of San Francisco Working Paper 2017-01. Available at https://doi.org/10.24148/wp2017-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf11b7-9f1e-4d9a-924b-aea88657d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.frbsf.org/economic-research/indicators-data/daily-news-sentiment-index/\n",
    "\n",
    "https://www.frbsf.org/economic-research/publications/working-papers/2017/01/\n",
    "\n",
    "https://medium.com/@financialimagineer/the-market-efficiency-paradox-949d9ebd62a9\n",
    "\n",
    "https://www.kaggle.com/code/ralarellanolopez/bitcoin-s-market-efficiency-analysis\n",
    "\n",
    "https://digitalcommons.usu.edu/cgi/viewcontent.cgi?article=2641&context=gradreports\n",
    "\n",
    "https://www.youtube.com/watch?v=L6zk0E6YApw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bfc5db-431f-4fcc-a26a-5b110cce0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.vantagemarkets.com/academy/market-events-2022/\n",
    "\n",
    "https://contentworks.agency/10-times-the-financial-markets-shocked-us-in-2022/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1b2e7-bb43-4d7b-86a9-fefa7949034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deeper Dive into Potential Inefficiencies\n",
    "\n",
    "### US High Yield Bonds:\n",
    "- **Market Composition**: This market consists of companies that are more susceptible to economic downturns and have higher default rates. As a result, behavioral biases might play a role, with investors reacting emotionally to news, thereby creating momentum in returns.\n",
    "  \n",
    "- **Liquidity**: High yield bonds are generally less liquid than investment-grade bonds. Less liquidity can lead to price discrepancies and create short-term momentum as prices adjust to new information.\n",
    "\n",
    "### US Equities:\n",
    "- **Market Dynamics**: Given the vast diversity of the US equities market, different sectors may respond differently to macroeconomic changes, leading to potential mean reversion as sectors cycle through periods of outperformance and underperformance.\n",
    "\n",
    "- **Information Flow**: The sheer volume of news and data available can sometimes lead to overreactions, causing short-term inefficiencies which later correct, leading to mean reversion.\n",
    "\n",
    "### US Leveraged Loans:\n",
    "- **Credit Risks**: Leveraged loans are typically extended to entities with lower credit ratings, meaning they're riskier. This heightened risk can introduce pricing inefficiencies as investors constantly reassess default probabilities.\n",
    "  \n",
    "- **Covenant Structures**: Leveraged loans often come with specific covenants that can influence the loan's risk profile. Changes or breaches in these covenants can lead to pricing discrepancies as the market reacts.\n",
    "  \n",
    "- **Secondary Market Dynamics**: Leveraged loans are traded in secondary markets which can be less liquid than traditional bond markets. This illiquidity can lead to temporary price inefficiencies.\n",
    "  \n",
    "- **Economic Sensitivity**: Companies that take on leveraged loans are often more sensitive to economic downturns. As a result, any potential economic disruptions can lead to sharper price adjustments in this market compared to more stable debt instruments.\n",
    "### Oil:\n",
    "- **Geopolitical Factors**: The oil market is greatly influenced by geopolitical events. Supply disruptions, conflicts, and OPEC decisions can introduce short-term momentum or mean reversion tendencies.\n",
    "  \n",
    "- **Supply/Demand Dynamics**: The oil market can often overreact to perceived shifts in supply or demand, leading to inefficiencies.\n",
    "\n",
    "### US Real Estate:\n",
    "- **Illiquidity**: Real estate is inherently less liquid than stocks or bonds. This illiquidity can lead to inefficiencies, especially during periods of economic stress when sellers might need to offload properties at sub-optimal prices.\n",
    "  \n",
    "- **Information Asymmetry**: Real estate markets often suffer from information asymmetry where buyers and sellers have different levels of information about a property. This can lead to inefficiencies in pricing.\n",
    "\n",
    "By understanding the underlying reasons for inefficiencies, investors can make more informed decisions and potentially capitalize on temporary market dislocations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3405803-0c25-4893-a401-a206d5a43d65",
   "metadata": {},
   "source": [
    "### Not Scaling Explanation\n",
    "XGBoost, like many tree-based algorithms, is invariant to the scale of numerical features. This means that scaling the input features generally doesn't impact the predictive power of the model. Here's an explanation of why XGBoost might perform better without scaling and why one might choose to leave them unscaled:\n",
    "\n",
    "\"Tree-based models, such as XGBoost, split the data based on feature values, seeking to differentiate the target variable regardless of the actual scale of the data. Thus, whether a feature's value is in the range of 0 to 1 or 0 to 1,000 doesn't fundamentally change the decision-making process of the tree. In contrast, algorithms that rely on distances between data points, like k-means clustering or k-nearest neighbors, or models that assume a certain distribution or scale, like linear regression, often benefit significantly from scaling.\n",
    "\n",
    "In our case, XGBoost performed better without scaling. This could be due to several reasons. First, the raw values might carry intrinsic information that's somehow lost or muddled during scaling. Second, any small noise or variance in the scaled data can be amplified, potentially introducing inconsistencies in the model's predictions. Lastly, not scaling ensures that our model's outputs and feature importance values are interpretable in their original units, which can be advantageous when explaining results to stakeholders or making real-world decisions.\n",
    "\n",
    "Given the superior performance and interpretability advantages, we chose to leave the features unscaled for our XGBoost model.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "User\n",
    "ok, write i nice summary for my article discussing how we're going to test the strong form efficinecy and why transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3416db87-eaee-47ad-bd0e-031b0d03f61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d85c07e-5a98-4f82-80d5-4b272f850d66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Scott Morgan\\\\Documents\\\\GitHub\\\\ml-market-efficiency'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5119c-fc55-4c41-8f52-8718b9f8a100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blpapi",
   "language": "python",
   "name": "blpapi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
